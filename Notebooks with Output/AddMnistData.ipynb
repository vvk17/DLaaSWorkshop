{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AddMNistData\n",
    "In this notebook we will use a number of libraries including ibm_boto3 in order to download the mnist dataset and upload it to IBM Cloud Object Storage. We will do so using two different methods, one to train a model and store its results using Keras and another for Watson Studio's Neural Network Modeler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I - Cloud Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip\n",
      "Building wheels for collected packages: wget\n",
      "  Running setup.py bdist_wheel for wget ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/6d/98/29/61ccc41148f871009126c2e844e26f73eeb25e12cca92228a5\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install various libraries, ibm_boto3 allows Python developers to manage Cloud Object Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import wget\n",
    "import keras\n",
    "import numpy\n",
    "import pickle\n",
    "import warnings\n",
    "import ibm_boto3\n",
    "from keras.datasets import mnist \n",
    "from ibm_botocore.client import Config\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the endpoint we will use. You can find this information in \"Endpoint\" section of your Cloud Object Storage dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_credentials = {\n",
    "  \"****PLACE YOUR COS CREDENTIALS HERE\"\n",
    "}\n",
    "api_key = cos_credentials['apikey']\n",
    "service_instance_id = cos_credentials['resource_instance_id']\n",
    "auth_endpoint = 'https://iam.bluemix.net/oidc/token'\n",
    "service_endpoint = 'https://s3-api.us-geo.objectstorage.softlayer.net'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create Boto resource by providing type, endpoint_url and credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = ibm_boto3.resource('s3',\n",
    "                         ibm_api_key_id=api_key,\n",
    "                         ibm_service_instance_id=service_instance_id,\n",
    "                         ibm_auth_endpoint=auth_endpoint,\n",
    "                         config=Config(signature_version='oauth'),\n",
    "                         endpoint_url=service_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II Downloading MNIST data and upload it to COS buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the buckets we will use to store training data and training results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**: Bucket name has to be globally unique - we will use a timestamp to do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bucket \"mnist-data-1522195721.5449784\"...\n",
      "Creating bucket \"mnist-results-1522195721.5449784\"...\n"
     ]
    }
   ],
   "source": [
    "timestamp = str(time.time())\n",
    "buckets = ['mnist-data-' + timestamp, 'mnist-results-' + timestamp]\n",
    "for bucket in buckets:\n",
    "    if not cos.Bucket(bucket) in cos.buckets.all():\n",
    "        print('Creating bucket \"{}\"...'.format(bucket))\n",
    "        try:\n",
    "            cos.create_bucket(Bucket=bucket)\n",
    "        except ibm_boto3.exceptions.ibm_botocore.client.ClientError as e:\n",
    "            print('Error: {}.'.format(e.response['Error']['Message']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should have our buckets created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with Keras **MNIST** sample dataset. Let's download our training data and upload them to 'mnist-keras-data' bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell will create 'MNIST_KERAS_DATA' folder and download the file from link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://s3.amazonaws.com/img-datasets/mnist.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist.npz\r\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'MNIST_KERAS_DATA'\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "if not os.path.isfile(os.path.join(data_dir, os.path.join(link.split('/')[-1]))):\n",
    "    wget.download(link, out=data_dir)  \n",
    "        \n",
    "!ls MNIST_KERAS_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data files to created buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist.npz is uploaded.\n"
     ]
    }
   ],
   "source": [
    "bucket_name = buckets[0]\n",
    "bucket_obj = cos.Bucket(bucket_name)\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    with open(os.path.join(data_dir, filename), 'rb') as data: \n",
    "        bucket_obj.upload_file(os.path.join(data_dir, filename), filename)\n",
    "        print('{} is uploaded.'.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III Download MNIST as python pickle for Neural Network Modeler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate to Watson Machine Learning service on Bluemix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X,y), (X_test,y_test) = mnist.load_data()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=.166, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action**: Put authentication information from your instance of Watson Machine Learning service here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mnist-nnm-train.pkl', 'wb') as f:\n",
    "    pickle.dump((X_train, y_train), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('mnist-nnm-valid.pkl', 'wb') as f:\n",
    "    pickle.dump((X_valid, y_valid), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('mnist-nnm-test.pkl', 'wb') as f:\n",
    "    pickle.dump((X_test, y_test), f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos.create_bucket(Bucket='mnist-nnm-' + timestamp)\n",
    "cos.Bucket('mnist-nnm-' + timestamp).upload_file('mnist-nnm-train.pkl', 'mnist-nnm-train.pkl')\n",
    "cos.Bucket('mnist-nnm-' + timestamp).upload_file('mnist-nnm-valid.pkl', 'mnist-nnm-valid.pkl')\n",
    "cos.Bucket('mnist-nnm-' + timestamp).upload_file('mnist-nnm-test.pkl', 'mnist-nnm-test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon finishing there should be 4 total objects, one in the first bucket and 3 in the second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object key: mnist.npz, size: 11221.1kB\n",
      "\n",
      "Object key: mnist-nnm-test.pkl, size: 7666.2kB\n",
      "Object key: mnist-nnm-train.pkl, size: 38360.9kB\n",
      "Object key: mnist-nnm-valid.pkl, size: 7635.6kB\n"
     ]
    }
   ],
   "source": [
    "for obj in cos.Bucket('mnist-data-' + timestamp).objects.all():\n",
    "    print('Object key: {}, size: {:5.1f}kB'.format(obj.key, obj.size/1024))\n",
    "\n",
    "print()\n",
    "\n",
    "for obj in cos.Bucket('mnist-nnm-' + timestamp).objects.all():\n",
    "    print('Object key: {}, size: {:5.1f}kB'.format(obj.key, obj.size/1024))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
